<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"dybeta2021.github.io","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="2017年的时候为了处理不同信息源的因子如何放在一起估算因子载荷的问题花了几周的时间研究Tesnor-Regression算法。后来发现挖因子比权重算法重要的多，再后面就转向研究期权了，今天刚好有空，把旧文回顾一下，温故知新，现在想想这个算法也是蛮有意思的。">
<meta property="og:type" content="article">
<meta property="og:title" content="Tesnor-Ridge-Regression">
<meta property="og:url" content="https://dybeta2021.github.io/2021/05/20/tesnor_ridge_regression/index.html">
<meta property="og:site_name" content="子非鱼">
<meta property="og:description" content="2017年的时候为了处理不同信息源的因子如何放在一起估算因子载荷的问题花了几周的时间研究Tesnor-Regression算法。后来发现挖因子比权重算法重要的多，再后面就转向研究期权了，今天刚好有空，把旧文回顾一下，温故知新，现在想想这个算法也是蛮有意思的。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://dybeta2021.github.io/2021/05/20/tesnor_ridge_regression/mosaic.png">
<meta property="og:image" content="https://dybeta2021.github.io/2021/05/20/tesnor_ridge_regression/%E5%BC%A0%E9%87%8F%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%84%E5%9B%BE.PNG">
<meta property="og:image" content="https://dybeta2021.github.io/2021/05/20/tesnor_ridge_regression/%E5%9B%A0%E5%AD%90%E5%BC%A0%E9%87%8F%E5%88%86%E8%A7%A3%E9%87%8D%E6%9E%84.PNG">
<meta property="og:image" content="https://dybeta2021.github.io/2021/05/20/tesnor_ridge_regression/CP%E5%88%86%E8%A7%A31-1499064.png">
<meta property="og:image" content="https://dybeta2021.github.io/2021/05/20/tesnor_ridge_regression/GDR.PNG">
<meta property="article:published_time" content="2021-05-20T08:02:55.000Z">
<meta property="article:modified_time" content="2021-05-20T02:37:25.000Z">
<meta property="article:author" content="稻草人">
<meta property="article:tag" content="MultiFactor">
<meta property="article:tag" content="LinearRegression">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://dybeta2021.github.io/2021/05/20/tesnor_ridge_regression/mosaic.png">

<link rel="canonical" href="https://dybeta2021.github.io/2021/05/20/tesnor_ridge_regression/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Tesnor-Ridge-Regression | 子非鱼</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">子非鱼</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>站点地图</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/dybeta2021" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://dybeta2021.github.io/2021/05/20/tesnor_ridge_regression/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="稻草人">
      <meta itemprop="description" content="风物长宜放眼量">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="子非鱼">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Tesnor-Ridge-Regression
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-05-20 16:02:55 / 修改时间：10:37:25" itemprop="dateCreated datePublished" datetime="2021-05-20T16:02:55+08:00">2021-05-20</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Math/" itemprop="url" rel="index"><span itemprop="name">Math</span></a>
                </span>
            </span>

          
            <div class="post-description">2017年的时候为了处理不同信息源的因子如何放在一起估算因子载荷的问题花了几周的时间研究Tesnor-Regression算法。后来发现挖因子比权重算法重要的多，再后面就转向研究期权了，今天刚好有空，把旧文回顾一下，温故知新，现在想想这个算法也是蛮有意思的。</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <ul>
<li><a
target="_blank" rel="noopener" href="https://github.com/dybeta2021/dyblog/tree/main/Tensor%20Ridge%20Regression">Tensor
Ridge Regression</a></li>
</ul>
<p>本文先简单归纳《A Tensor-Based Information Framework for Predicting
the Stock Market
》中介绍的一种探索多维数组（张量）表征多因子并计算不同因子抽象关系并降低维度的方法，然后归纳《Tensor
Learning for Regression 》中介绍的Tesnor Ridge
Regression算法，通过CP分解张量降低需要估计回归系数（因子收益率）的数量。</p>
<p>股票趋势受各种高度相关信息的强烈影响，这通常涵盖经济学、政治学和心理学等多方面研究。传统的有效市场假说（EMH）指出，股价总是由“理性”的投资者驱动，股价等于公司预期未来现金流的理性现值。与市场有关的新信息可能会改变投资者的期望，并导致股价波动，这种对信息反应的分歧导致股价的实际价格与内在价值之间的差异。竞争市场参与者导致股价波动周围的股价内在价值，即新信息对股价产生复杂的影响。然而，股价并不严格遵循随机游走，行为金融研究将股票趋势的非随机性归结为投资者由于认知和情绪偏见的对不利消息的过度反应。虽然传统金融和行为金融均认为新的信息对股票趋势产生复杂影响。</p>
<p>A股市场刨除内幕等操作方法为，一般分为基本面分析和技术分析两类。基本面分析通常通过构建经济、商业和市场行业等多信息源的数据与股票未来走势之间的关系来预测股票趋势，即国家整体经济，行业条件，公司的财务状况和管理层能力等因素，可以深度拆解股价未来走势。技术分析通过历史股票趋势预测股价未来走势，技术分析流派认为股价市场是周期性或者类周期性的，并且具有特定的模式，这些模式随着时间的推移而重复出现。</p>
<p>受到移动互联网的影响，股票信息迅速更新并以前所未有的速度传播，并且通常在正式统计报告之前向投资者对投资者产生影响。用户参与社交媒体（包括评论，评分和投票）的变化可以更快速地互动交换信息。这种情况可能导致群体投资行为，因为投资者的决策倾向于受同行的情绪影响。目前常见的做法是使用NLP量化新闻和社交网络的新信息提供定性信息，如舆情指数。影响股票趋势的信息是多方面的且互相影响的，反映到因子数据方面就是存在多重共线性。</p>
<p>传统的线性回归将多个信息源（模式）的特征（无量纲因子）连接到一个复合特征的向量中处理。由于维度灾难和多重共线性，这种做法限制了多因子模型囊括因子数量。此外使用马赛克拼接(mosaic
approach)通常包含各种信息源的混合和交互，但级联向量假设每个信息膜是是相互独立的。如下图这里，矩阵用于建模简化的投资信息源，其中每行代表一种信息模式，如企业信息、事件信息或情绪信息。根据马赛克拼接信息结构，特征模式可以存在于不同信息源（行）或不同模式之间。</p>
<p><img src="mosaic.png" width="100%" height="100%" title="mosaic" alt="mosaic"/></p>
<p>如图右所示，删除马赛克信息结构并将各种信息模式的特征连接成一个复合特征向量，级联向量方法忽略了各种信息模式之间的固有链路导致明显的相似性的出现。除了在一个信息矩阵中捕获各种模式之间的这些静态互连之外，重要的是在一系列信息矩阵之间识别和强调各种模式之间的动态连接。例如，在不同时间发布的两篇新闻文章可能是文本上不相似的，但两者可能包含有关相同股价的有利信息。此外，在这两个时间点的相应的企业特定数据可能相似，可能表明良好的投资机会。在这种情况下，可以通过相应的基本面数据的相似性来增强不同文本信息处理、舆情指数相似性。捕捉，推论和加强各种信息模式之间的动态关系将有利于提高对股票趋势的预测能力。</p>
<p>此文中应用基于张量的信息框架来捕捉新信息与股票走势之间的非线性关系。该框架使用全局维数降低算法和基于张量的回归获取多信息源之前的非线性关系，并研究这些信息源对股票趋势的影响。影响股票走势的各种信息因素，在过去已被广泛研究。
传统金融主要集中在企业特定因素的长期影响上，而现代行为金融主要集中在公众情绪和当前事件造成的短期影响。此文试图通过建立多维度模型处理各种信息源对股票趋势的共同影响。这里将信息按照信息源分成三类：(a)
企业特定模型信息模式：主要指企业的基本面和量价因子；(b)
事件特定模式：主要指新闻资讯对股价走势的影响，受新信息的影响，股票投资者不断更新他们公司经营情况等基本面及股价走势的看法或非理性投资者的情绪。具体来说，可以对相关新闻文章使用词向量表示，其中每个条目是名词和情感词的加权。(c)
情绪特定模式：发达的社交媒体使一部分投资者潜意识群体和情绪的影响，这可能导致投资者的群体性行为。</p>
<p><img src="%E5%BC%A0%E9%87%8F%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%84%E5%9B%BE.PNG" width="100%" height="100%" title="张量预测结构图" alt="张量预测结构图"/></p>
<p>可以使用张量处理多信息源信息的相互关联特性，这里使用三阶张量表示三种不同的信息模式。将三种不同信息源的信息（因子）填入张量不同维度（形成一个稀疏张量），如下图，应用张量分解和重构来降低维度，加强不同信息模式的内在联系。通过使用张量分解子空间中的因子矩阵来实现从张量序列的几何结构中识别出几个信息源之间的深层关联。使用<span
class="math inline">\(Tucker\)</span>分解因子张量<span
class="math inline">\(\mathcal{X}_i = \mathcal{C}_i \times_1 U^{i_1}
\times_2 U^{i}_2 \times_3U^{i}_3\)</span>
使用GDR算法计算因子矩阵修正矩阵并重新合成信息因子张量<span
class="math inline">\(\bar{\mathcal{X}_i} = \mathcal{C}_i \times_1(V^T_1
U^{i_1}) \times_2 (V^T_2 U^{i}_2) \times_3(V^T_3U^{i}_3)\)</span>
。这样多信息源的原始因子稀疏张量转化为低维度的密集因子张量。<span
class="math inline">\(\mathcal{X}\in\mathbb{R}^{I_1 \times I_2 \times
I_3} \to \bar{\mathcal{X}} \in \mathbb{R}^{J_1 \times J_2 \times
J_3}\)</span></p>
<p><img src="%E5%9B%A0%E5%AD%90%E5%BC%A0%E9%87%8F%E5%88%86%E8%A7%A3%E9%87%8D%E6%9E%84.PNG" width="100%" height="100%" title="因子张量分解重构" alt="因子张量分解重构"/></p>
<p>尽管通过前述步骤分解-重构技术可以大大压缩稀疏的因子张量的体积规模，然而使用传统的方法如张量向量化估计张量回归权重系数依然具有较大的难度。例如对形状为[30,
30,
10]因子张量进行回归，张量向量化的处理方法导致权重系数张量需要估计<span
class="math inline">\(30 \times 30 \times 10 = 9000\)</span>
个权重参数（标量），对于传统多因子模型这显然是不实用。可以通过使用张量分解的方法（如CP分解）大幅度降低所需估计的权重系数的数量，从而将张量回归方法嵌入传统多因子体系中。例如在《Tensor
Learning for Regression 》介绍的tensor ridge
regression。对上面的假设，使用CP分解权重张量之后，例如分解为4个坐标基（秩一张量）累加和则只需要估计<span
class="math inline">\((30 + 30 +10) \times 4 =280\)</span>
个权重参数。</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
y_i &amp;= \left&lt;\mathcal{X},\mathcal{W} \right &gt; + b\\
&amp;=\left&lt; \mathcal{X},  \sum\limits^R_{r=1} u^{(1)}_r \circ
u^{(2)}_r \circ \cdots \circ u^{(M)}_r \right&gt; + b\\
&amp;=\sum\limits^R_{r=1} \left&lt; \mathcal{X},   u^{(1)}_r \circ
u^{(2)}_r \circ \cdots \circ u^{(M)}_r \right&gt; + b\\
&amp;=\sum \limits ^{R} _{r=1} \mathcal{X} \prod^{M_{k=1}} \times_k
u^{(k)}_r
\end{aligned}
\end{equation}\]</span></p>
<p>对一组给定的有标签训练集<span class="math inline">\(\{\mathcal{X}_i
,y_i \}^N_{i=1}\)</span> 其中<span class="math inline">\(\mathcal{X}_i =
\mathbb{R}^{I_1 \times \cdots I_M}\)</span> 为<span
class="math inline">\(M-mode\)</span> 张量，<span
class="math inline">\(y_i\)</span>
为对应的标量标签。评估权重参数的目标函数可以写作</p>
<p><span class="math display">\[\mathcal{L} (\mathcal{W},b) =
\frac{1}{2} \sum\limits^{N}_{i=1} l(y_i, f(\mathcal{X}_i, \Theta)) +
\frac{1}{2} \psi(\Theta)\]</span></p>
<p><span class="math inline">\(l(\cdot)\)</span> 偏差损失函数，<span
class="math inline">\(\psi (\cdot)\)</span>
惩罚函数。当使用响应变量和标签差最小平方和作为损失函数的时候就是张量形式的最小二乘估计，添加2范数作为惩罚函数便得到张量形式的岭回归。在不使用惩罚项修正权重参数的时候，即张量形式的最小二乘回归中偏置<span
class="math inline">\(b\)</span> 直接使用上式得出，在使用<span
class="math inline">\(L2-norm\)</span>
修正权重参数的时候由于偏置权重参数同时被修正，这导致常数偏置项的错误。所以TRR的常数偏置应该使用无偏置回归的残差平均值获得，也就是使用无偏置回归模型的残差平均值计算偏置，但哑变量表示的行业收益率可以直接计算。</p>
<h1 id="附录">附录</h1>
<h2 id="向量-线性回归">1、向量-线性回归</h2>
<h3 id="符号">1.1、符号</h3>
<p><span class="math inline">\(x\)</span> 列向量</p>
<p><span class="math inline">\(x^T\)</span> 行向量</p>
<p><span class="math inline">\(X\)</span> 矩阵</p>
<p><span class="math inline">\(I\)</span> 单位矩阵</p>
<p><span class="math inline">\(\mathcal{X}\)</span> 张量</p>
<p><span class="math inline">\(||\cdot||_2\)</span> L2范数</p>
<h3 id="least-squares-regression">1.2、Least Squares Regression</h3>
<p>传统线性回归模型可以写作：</p>
<p><span class="math inline">\(y = f(X) + \varepsilon\)</span></p>
<p>使用线性模型拟合得到的估计值模型为:</p>
<p><span class="math inline">\(\hat{ y} = f(X) = X \beta = \beta_0 1 +
\beta_1 x_1 +\cdots + \beta_k x_k\)</span></p>
<p>其中</p>
<p><span class="math inline">\(y \in \mathbb{R}^n\)</span></p>
<p><span class="math inline">\(X \in \mathbb{R}^{n \times (k
+1)}\)</span></p>
<p><span class="math inline">\(\varepsilon \in \mathbb{R}\)</span></p>
<p><span class="math inline">\(\beta \in \mathbb{R}^{k+1}\)</span></p>
<p>注：下文<span class="math inline">\(k+1\)</span>简写为<span
class="math inline">\(k\)</span></p>
<p>使用最小二乘损失函数获取权重参数</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\mathcal{L} &amp;=Loss\ Function \\
&amp;=RSS(\beta) \\
&amp;= ||y-X\beta||^2_2  \\
&amp;= (y-X\beta)^T(y-X\beta)\\
&amp;=y^T y -(X\beta)^Ty-y^TX\beta + (X\beta)^T(X\beta)\\
&amp;=(X\beta)^T(X\beta) - 2(X\beta)^Ty + y^Ty\\
&amp;=\beta^TX^TX\beta - 2 \beta^T X^T y + y^Ty
\end{aligned}
\end{equation}\]</span></p>
<p>其中 <span class="math inline">\(y^TX\beta = scalar = (X\beta
)^Ty\)</span></p>
<p>令<span class="math inline">\(A = X^TX \in \mathbb{R}^{k\times
k}\)</span></p>
<p><span
class="math inline">\(\frac{\partial{\beta^TX^TX\beta}}{\partial{\beta}}=\frac{\partial{\beta^TA\beta}}{\partial{\beta}}\\
= (A + A^T)\beta\\ = (X^TX + (X^TX)^T)\beta\\=2X^TX\beta\)</span></p>
<p><span
class="math inline">\(\frac{\partial{\beta^TX^Ty}}{\partial{\beta}}
=\\\frac{\partial{\beta^T}}{\partial{\beta}}(X^Ty) + \beta^T
\frac{\partial{X^Ty}}{\partial{y}}\\=I(X^Ty +
\beta^T0)\\=X^Ty\)</span></p>
<p><span class="math inline">\(\frac{\partial{\mathcal{L}
}}{\partial{\beta}} = 2X^TX\beta - 2X^Ty\)</span></p>
<p>由于这是凸优化问题，令<span
class="math inline">\(\frac{\partial{J}}{\partial{\beta}}=0\)</span>
，则得到权重参数<span class="math inline">\(\beta\)</span> 的估计值</p>
<p><span class="math inline">\(\hat{\beta} =
(X^TX)^{-1}X^Ty\)</span></p>
<p>最小二乘回归根据预测变量拟合响应变量为</p>
<p><span class="math inline">\(\hat{y} = X\hat{\beta} = X
(X^TX)^{-1}X^Ty\)</span></p>
<h3 id="proof-fracpartialbetatabetapartialbeta-a-atbeta">1.3、 Proof:
<span
class="math inline">\(\frac{\partial{\beta^TA\beta}}{\partial{\beta}} =
(A + A^T)\beta\)</span></h3>
<ul>
<li>微分法</li>
</ul>
<p>设<span class="math inline">\(u(\beta)=\beta^TA\beta, \ \ h=\Delta
\beta\)</span></p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
u(\beta+h)&amp;=(\beta+h)^TA(\beta+h)\\
&amp;=\beta^TA\beta+h^TA\beta+\beta^TAh+h^TAh\\
&amp;=u(\beta)+\beta^T(A+A^T)h+u(h) \\
&amp;=u(\beta) + h^T(A+A^T)\beta +u(h)
\end{aligned}
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(h^TA\beta= 1\times1 \ matrix=
scalar=\beta^TA^Th\)</span></p>
<p>设<span class="math inline">\(r_\beta(h)=u(h)=h^TAh\)</span>
，当<span class="math inline">\(h\to0\)</span> 则<span
class="math inline">\(r_\beta(h)=o(\|h\|)\)</span> 这证明<span
class="math inline">\(u\)</span>在<span
class="math inline">\(\beta\)</span>处的微分是线性函数。则</p>
<p><span class="math inline">\(\because \ u(\beta+\Delta \beta) -
u(\beta)= h^T(A+A^T)\beta +u(h)\)</span></p>
<p><span class="math inline">\(\therefore \ \nabla u(\beta) = I (A +
A^T)\beta + o(\|h\|)\)</span></p>
<p>即 <span
class="math inline">\(\frac{\partial{\beta^TA\beta}}{\partial{\beta}} =
(A + A^T)\beta\)</span></p>
<ul>
<li>元素法</li>
</ul>
<p>设<span class="math inline">\(A\in \mathbb{R}^{k\times k}\)</span> 即
<span class="math inline">\(A = [a_{i,j}]\)</span> , <span
class="math inline">\(\beta \in \mathbb{R}^k\)</span></p>
<p><span class="math inline">\(u =\beta^T A \beta = \sum \limits
^k_{i=1} \sum \limits ^k_{j=1} a_{i,j} \beta_i \beta_j\)</span></p>
<p>根据元素对列向量求导规则</p>
<p><span class="math inline">\(\frac{\partial{y_p}}{\partial{x}} =
[\frac{\partial{y_p}}{\partial{x_1}}, \cdots
,\frac{\partial{y_p}}{\partial{x_k}}]^T\)</span></p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\frac{\partial{u}}{\partial{\beta_p}}&amp;=\frac{\sum\limits ^k_{i=1}
\sum\limits^{k}_{j=1} \partial{a_{i,j}\beta_i
\beta_j}}{\partial{\beta_p}}\\
&amp;=\frac{\sum\limits ^k_{i=1, i\neq p} \sum\limits^{k}_{j=1}
\partial{a_{i,j}\beta_i \beta_j}}{\partial{\beta_p}} +
\frac{\sum\limits^{k}_{j=1} \partial{a_{p,j}\beta_p
\beta_j}}{\partial{\beta_p}}\\
&amp;=\frac{\sum\limits ^k_{i=1, i\neq p} \sum\limits^{k}_{j=1, j\neq p}
\partial{a_{i,j}\beta_i \beta_j}}{\partial{\beta_p}} + \frac{\sum\limits
^k_{i=1, i\neq p}  \partial{a_{i,p}\beta_i \beta_p}}{\partial{\beta_p}}
+ \frac{\sum\limits^{k}_{j=1,j\neq p} \partial{a_{p,j}\beta_p
\beta_j}}{\partial{\beta_p}} + \frac{\partial{a_{p,p}\beta_p
\beta_p}}{\partial{\beta_p}}\\
&amp;=0 + \sum \limits ^{k}_{i=1,i\neq p} a_{i,p} \beta_i + \sum \limits
^k _{j=1, j\neq p} a_{p,j} \beta_j + 2a_{p,p}\beta_p \\
&amp;= \sum \limits^k_{i=1}a_{i,p}\beta_i + \sum
\limits^k_{j=1}a_{p,j}\beta_j \\
&amp;= (\beta^TA)_p + (A\beta)_p\\
&amp;=(A^T\beta)_p + (A\beta)_p
\end{aligned}
\end{equation}\]</span></p>
<p>其中<span class="math inline">\((\beta^TA)_p\)</span> 是行向量<span
class="math inline">\(\beta^TA\)</span>的第<span
class="math inline">\(p\)</span>个分量,<span
class="math inline">\((A\beta)_p\)</span> 是列向量<span
class="math inline">\(A\beta\)</span>的第<span
class="math inline">\(p\)</span>的元素。</p>
<h3 id="ridge-regression">1.4、Ridge Regression</h3>
<p>当输入因子共线性严重的时候适用岭回归，岭回归是对最小二乘回归的一种补充，它损失了无偏性，来换取高的数值稳定性，从而得到较高的计算精度。</p>
<p>在最小二乘回归中最小化损失函数</p>
<p><span class="math inline">\(\mathcal{L} = \sum (y-(\beta_0 +
\sum\limits^k_{i=1} \beta_i x_i))^2\)</span></p>
<p>求得<span class="math inline">\(\beta\)</span>
权重系数。在岭回归中最小化目标函数则是</p>
<p><span class="math inline">\(\mathcal{L} = \sum (y-(\beta_0 +
\sum\limits^k_{i=1} \beta_i x_i))^2 + \alpha \sum \limits
^k_{i=1}\beta_i^2\)</span></p>
<p>通过添加对权重参数的2范数惩罚项，抑制<span
class="math inline">\(\beta\)</span>过大数值，即抑制响应变量对单个预测变量的敏感性。</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
J^{ridge} &amp;=RSS(\beta^{ridge}) \\
&amp;= \min \limits_{\beta}\|y-X\beta\|^2 + \lambda\|W\|^2_2 \\
&amp;=(y-X\beta)^T(y-X\beta) + \lambda \beta^T\beta\\
&amp;=\beta^TX^TX\beta - 2 \beta^T X^T y + y^Ty + \lambda \beta^T\beta
\end{aligned}
\end{equation}\]</span></p>
<p>令<span
class="math inline">\(\frac{\partial{J^{ridge}}}{\partial{\beta}} =
2X^TX\beta - 2X^Ty +2\lambda \beta=0\)</span></p>
<p>则<span class="math inline">\(\hat{\beta}^{ridge} = (X^TX + \lambda
I)^{-1}X^Ty\)</span></p>
<p>对比 <span class="math inline">\(\hat{\beta}^{ols} =
(X^TX)^{-1}X^Ty\)</span></p>
<p>对<span class="math inline">\(OLS\)</span>当<span
class="math inline">\(X\)</span>不是列满秩（列满秩：因子矩阵列线性无关）或输入因子存在较强的多重共线性，<span
class="math inline">\(X^TX\)</span>的行列式接近于0，即<span
class="math inline">\(X^TX\)</span>接近奇异。计算<span
class="math inline">\((X^TX)^{-1}\)</span>误差较大。当<span
class="math inline">\(X^TX\)</span>的行列式接近于0时，ridge regression
将其主对角元素都加上一个<span
class="math inline">\(\lambda\)</span>数，可以降低矩阵奇异风险。当<span
class="math inline">\(\lambda\)</span>（收缩量或惩罚系数）增加时，偏差增加，方差减小。</p>
<p>常数项使用<span
class="math inline">\(L2\)</span>惩罚项将导致回归出现错误。将输入因子矩阵按列标准化之后，截距估计为<span
class="math inline">\(\beta_0 = \bar{y}\)</span> 。所以通常使用ridge
regression 回归的时候先对输入因子矩阵<span
class="math inline">\(X\)</span>
标准化处理，并不包含截距（偏置）输入。对于分类独立预测变量（哑变量），<span
class="math inline">\(ridge \ regression\)</span> 和<span
class="math inline">\(lass \ regression\)</span> 同样有效。</p>
<p>岭回归通过对权重系数的大小施加惩罚来规范线性回归。权重系数朝着零向彼此收缩，但是，当这种情况发生时，如果预测变量没有相同的尺度，那么收缩是不公平的。两个具有不同尺度的预测变量将对惩罚项有不同的贡献，因为惩罚项是所有系数的平方和。为了避免这种问题，很多时候，独立变量的中心和缩放是为了具有方差1。但是二进制变量（哑变量）不一定代表高斯/正态分布。当将它们转换为“normal”值，其中mean
= 0和std.dev =
1时，不会创建基本的正态分布，您可以应用可能不符合的假设。</p>
<h2 id="张量-线性回归">2、张量-线性回归</h2>
<h3 id="符号-1">2.1 符号</h3>
<p><span class="math inline">\(x_i\)</span> 标量</p>
<p><span class="math inline">\(x\)</span> 列向量</p>
<p><span class="math inline">\(x^T\)</span> 行向量</p>
<p><span class="math inline">\(X\)</span> 矩阵</p>
<p><span class="math inline">\(I\)</span> 单位矩阵</p>
<p>张量 <span class="math inline">\(\mathcal{X} = \mathbb{R}^{I_1 \times
\cdots I_M}\)</span></p>
<p>张量的d-mode矩阵 <span class="math inline">\(mat_d(\mathcal{X}) =
X_{(d)} \in \mathbb{R}^{I_d \times (I_1 \cdots I_{d-1}I_{d+1}\cdots
I_M)}\)</span></p>
<p>张量向量化<span class="math inline">\(vec(\mathcal{X})\)</span></p>
<h3 id="tensor-ridge-regression">2.2 Tensor Ridge Regression</h3>
<p>矢量空间中的经典线性预测器由下式给出</p>
<ul>
<li><span class="math inline">\(y_i = f(x) = \left&lt; x,\beta
\right&gt; +b\)</span></li>
</ul>
<p>这里是响应变量为标量的线性回归模型。将上述经典线性预测器从矢量空间扩展到张量空间</p>
<ul>
<li><span class="math inline">\(y_i = f(\mathcal{X}) =
\left&lt;\mathcal{X},\mathcal{W}\right&gt;+b\)</span></li>
</ul>
<p>当使用张量向量化处理如 <span class="math inline">\(\left&lt;
vec(\mathcal{X}, vec(\mathcal{W}))
\right&gt;\)</span>的时候上述两式完全相同，然而在向量预测器中对于多维度或者多信息源的预测变量将输入数据简单的拼接成向量，容易出现过拟合和高计算复杂度的问题。尽管可以通过使用无监督维度降低算法（如PCA）处理输入因子向量，但这通常导致难以建立明晰的预测变量对响应变量的解释。这里使用张量CP分解方法，通过将权重参数张量<span
class="math inline">\(\mathcal{W}\)</span>的分解为多个秩一张量相加来执行特征选择或权重系数的维数降低并同时捕获多维度预测变量之间的深层抽象关系。通过对权重张量的CP分解可以线性预测器可以改写为：</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
y_i &amp;= \left&lt;\mathcal{X},\mathcal{W} \right &gt; + b\\
&amp;=\left&lt; \mathcal{X},  \sum\limits^R_{r=1} u^{(1)}_r \circ
u^{(2)}_r \circ \cdots \circ u^{(M)}_r \right&gt; + b\\
&amp;=\sum\limits^R_{r=1} \left&lt; \mathcal{X},   u^{(1)}_r \circ
u^{(2)}_r \circ \cdots \circ u^{(M)}_r \right&gt; + b\\
&amp;=\sum\limits^R_{r=1} \left&lt; \mathcal{X}, \mathcal{U}_r
\right&gt; +b
\end{aligned}
\end{equation}\]</span></p>
<p>这里使用CP分解将权重参数张量<span
class="math inline">\(\mathcal{W}\)</span>分解为R个秩一张量<span
class="math inline">\(\mathcal{U}_r\)</span>，如果将这R个秩一张量看做一组坐标基，则每个秩一张量都可以视为一个坐标基（如右手坐标系的一个维度），这样对R个秩一张量的累加和就可以看做类比于空间几何投影概念，也就是样本张量<span
class="math inline">\(\mathcal{X}\)</span>
投影到具有R个坐标基的空间中。类似于<span
class="math inline">\(PCA\)</span>
，使用多个投影的减少沿着一个方向进行投影时发生的信息丢失。</p>
<p>这种方法可以有效降低估计权重参数数量，
例如输入预测变量为的形状设定为[30, 30,
10]的形状，则对应的权重张量需要估计<span class="math inline">\(30 \times
30 \times 10 = 9000\)</span>
个权重参数（标量），使用CP分解权重张量之后，例如分解为4个坐标基（秩一张量）累加和则只需要估计<span
class="math inline">\((30 + 30 +10) \times 4 =280\)</span>
个权重参数。</p>
<h3 id="内积-inner-product">2.3 内积 Inner Product</h3>
<p>向量<span class="math inline">\(x,w \in \mathbb{R}^N\)</span></p>
<ul>
<li><span class="math inline">\(\left&lt; x, w\right&gt; =
\sum\limits^N_{i=1}x_iw_i=x^Tw\)</span></li>
</ul>
<p>矩阵 <span class="math inline">\(X,W \in
\mathbb{R}^{m,n}\)</span></p>
<ul>
<li><span class="math inline">\(\left&lt;X,W
\right&gt;\\=\sum\limits^{m}_{i=1}\sum\limits^{n}_{j=1}x_{i,j}w_{i,j}
\\= vec(X)^Tvec(W) \\=
vec(W)^Tvec(X)\\=\left&lt;vec(X),vec(W)\right&gt;\)</span></li>
</ul>
<p>张量<span class="math inline">\(\mathcal{X}, \mathcal{W} \in
\mathbb{R}^{I_1 \times \cdots \times I_M}\)</span></p>
<ul>
<li><span class="math inline">\(\left&lt; \mathcal{X},\mathcal{W}
\right&gt;\\ = \sum\limits^{I_1}
_{i_1=1}\cdots\sum\limits^{I_M}_{i_M=1}x_{i_1,\cdots,i_M}w_{i_1,\cdots,i_M}\\=vec(\mathcal{X})^Tvec(\mathcal(W)\\=\left&lt;vec(\mathcal{X},\mathcal{W})
\right&gt;\)</span></li>
</ul>
<p>根据元素计算可得</p>
<ul>
<li><span class="math inline">\(\left&lt; vec(\mathcal{X}),
vec(\mathcal{W})\right&gt; =\left&lt;X_{(d)}, W_{(d)}\right&gt; =
\left&lt; \mathcal{X},\mathcal{W} \right&gt;\)</span></li>
</ul>
<h3 id="矩阵内积与迹运算">2.4 矩阵内积与迹运算</h3>
<p>设<span class="math inline">\(A=(a_{ij})\in\mathbb{R}^{m,n}\)</span>
，<span class="math inline">\(B=(b_{ij})\in\mathbb{R}^{m,n}\)</span>
，<span
class="math inline">\(C=B^TA=(c_{ij})\in\mathbb{R}^{n,n}\)</span> 。</p>
<ul>
<li><span class="math inline">\((c)_{ij}=\sum_{k=1}^m
b_{ki}a_{kj}\)</span></li>
<li><span class="math inline">\(\mathrm{tr}(B^TA)=\sum_{i=1}^n
c_{ii}=\sum_{i=1}^n\sum_{k=1}^m b_{ki}a_{ki}\)</span></li>
</ul>
<p>则<span class="math inline">\(\mathrm{tr}(B^TA) =
\left&lt;A,B\right&gt;= \text{tr}((B^TA)^T)=\text{tr}(A^TB)\)</span></p>
<p>设<span class="math inline">\(A=(a_{ij})\in\mathbb{R}^{m,n}\)</span>
，<span class="math inline">\(B=(b_{ij})\in\mathbb{R}^{m,n}\)</span>
，<span
class="math inline">\(C=AB^T=(c_{ij})\in\mathbb{R}^{m,m}\)</span> 。</p>
<ul>
<li><span class="math inline">\((c)_{ij}=\sum\limits_{k=1}^n
b_{ki}a_{kj}\)</span></li>
<li><span class="math inline">\(\mathrm{tr}(AB^T)=\sum\limits_{i=1}^m
c_{ii}=\sum\limits_{i=1}^m\sum\limits_{k=1}^n b_{ki}a_{ki}\)</span></li>
</ul>
<p>则<span class="math inline">\(\langle A,B\rangle = \mathrm{tr}(B^TA)
= \text{tr}((B^TA)^T)=\text{tr}(A^TB)\\= \mathrm{tr}(AB^T) =
\text{tr}((AB^T)^T)=\text{tr}(AB^T)\)</span></p>
<h3 id="frobenius-norm">2.5 Frobenius norm</h3>
<ul>
<li><span class="math inline">\(\| A\| = \sqrt{\sum\limits^m_{i=1}
\sum\limits^m_{j=1}\| a_{i,j}\|^2}\)</span></li>
<li><span class="math inline">\(\| A\|^2 = \sum\limits^m_{i=1}
\sum\limits^m_{j=1}\| a_{i,j}\|^2= \left&lt;A,A\right&gt; =
trace(AA^T)\)</span></li>
</ul>
<h3 id="cp分解">2.6 CP分解</h3>
<p><strong>canonical polyadic decomposition</strong></p>
<p><img src="CP%E5%88%86%E8%A7%A31-1499064.png" width="100%" height="100%" title="canonical polyadic decomposition" alt="canonical polyadic decomposition"/></p>
<ul>
<li><span class="math inline">\(\mathcal{W}\\ \approx [[ U^{(1)},
U^{(2)},\cdots U^{(M)}]] \\ \triangleq \sum\limits^R_{r=1} u^{(1)}_r
\circ u^{(2)}_r \circ \cdots \circ u^{(M)}_r\)</span></li>
</ul>
<p>其中 <span class="math inline">\(U^{(K)} = [u^{(k)}_1, \cdots,
u^{(k)}_r ]\)</span> 对张量 <span
class="math inline">\(\mathcal{W}\)</span> 的 <span
class="math inline">\(mode-d\)</span>展开矩阵 <span
class="math inline">\(W_{(d)}\)</span>，CP分解转换为：</p>
<ul>
<li><span class="math inline">\(W_{(d)}=U^{(d)} U^{(-d)T}\)</span></li>
</ul>
<p>其中</p>
<ul>
<li><span class="math inline">\(U^{(d)} \in \mathbb{R}^{I_d\times
R}\)</span></li>
<li><span class="math inline">\(U^{(-d)} \in \mathbb{R}^{(I_1 \cdots\
I_{d-1} I_{d+1}\cdots I_{M}) \times R}=\mathbb{R}^{S \times
R}\)</span></li>
<li><span class="math inline">\(U^{(-k)} = (U^{M}\odot \cdots \odot
U^{(d+1)}\odot U^{(d-1)}\odot\cdots \odot U^{(1)})\)</span></li>
</ul>
<h3 id="trr目标函数">2.7 TRR目标函数</h3>
<p>对一组给定的有标签训练集<span class="math inline">\(\{ \mathcal{X}_i
,y_i \}^N_{i=1}\)</span> 其中<span class="math inline">\(\mathcal{X}_i =
\mathbb{R}^{I_1 \times \cdots I_M}\)</span> 为<span
class="math inline">\(M-mode\)</span> 张量，<span
class="math inline">\(y_i\)</span>
为对应的标量标签。评估权重参数的目标函数可以写作</p>
<ul>
<li><span class="math inline">\(\mathcal{L} (\mathcal{W},b) =
\frac{1}{2} \sum\limits^{N}_{i=1} l(y_i, f(\mathcal{X}_i, \Theta)) +
\frac{1}{2} \psi(\Theta)\)</span></li>
</ul>
<p><span class="math inline">\(l(\cdot)\)</span> 偏差损失函数，<span
class="math inline">\(\psi (\cdot)\)</span>
惩罚函数。当使用响应变量和标签差最小平方和作为损失函数的时候就是张量形式的最小二乘估计，添加2范数作为惩罚函数便得到张量形式的岭回归。</p>
<p>使用CP分解将权重参数张量转换为一组矩阵</p>
<ul>
<li><span class="math inline">\(\mathcal{W} \approx [[ U^{(1)},
U^{(2)},\cdots U^{(M)}]]\)</span></li>
</ul>
<p>则目标函数转换为</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\mathcal{L} (\mathcal{W},b) &amp;= \mathcal{L} (\{U^{(1)},\cdots,U^{(M)}
\},b)+ \Phi(\mathcal{W})\\
&amp;=\frac{1}{2} \sum\limits^N_{i=1}(y_i -
\left&lt;\mathcal{X}_i,[[U^{(1)},\cdots,U^{(M)}]] \right&gt;-b)^2 +
\frac{\lambda}{2}\| [[U^{(1)},\cdots,U^{(M)}]] \|^2_{Fro}
\end{aligned}
\end{equation}\]</span></p>
<p>使用坐标下降法最小化这个目标函数，在每次迭代中，固定除了<span
class="math inline">\(U^{(j)}\)</span>
之外的其他权重矩阵，解决相对于权重参数矩阵集的一个子集<span
class="math inline">\(U^{(j)}\)</span>的凸优化问题。在每次迭代中，求解与模式投影相关联的权重张量分解参数矩阵<span
class="math inline">\(U^{(j)}\)</span>，同时保持其它模式的投影对应的参数矩阵<span
class="math inline">\(\{ U^{(k)}\}|^M_{k=1,k\neq
j}\)</span>固定。也就是在每迭代步最小化如下子目标函数：</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\mathcal{L}_{d} (U^{(d)},b) &amp;=l_d(U^{(d)},b) + \Phi_d(U^{(d)})\\
&amp;=\frac{1}{2} \sum\limits^N_{i=1}(y_i - \left&lt; X_{i(d)},W_{(d)}
\right&gt;-b)^2 +\frac{\lambda}{2}\| W_{(d)} \|^2_{Fro} \\
&amp;=\frac{1}{2} \sum\limits^N_{i=1}(y_i - \left&lt;X_{i(d)},U^{(d)}
U^{(-d)T}\right&gt;-b)^2 + \frac{\lambda}{2}\left&lt;U^{(d)}U^{(-d)T},
U^{(d)}U^{(-d)T} \right&gt;\\
&amp;=\frac{1}{2} \sum\limits^N_{i=1}(y_i - \left&lt;U^{(d)}
U^{(-d)T},X_{i(d)}\right&gt;-b)^2 +
\frac{\lambda}{2}\left&lt;U^{(d)}U^{(-d)T}, U^{(d)}U^{(-d)T}
\right&gt;  \\
&amp;=\frac{1}{2} \sum\limits^N_{i=1}(y_i - \text{Tr}(U^{(d)}
U^{(-d)T}X_{i(d)}^T)-b)^2 +
\frac{\lambda}{2}\text{Tr}(U^{(d)}U^{(-d)T}U^{(-d)}U^{(d)T} )
\end{aligned}
\end{equation}\]</span></p>
<p>其中权重参数张量和CP分解矩阵关系为 <span
class="math inline">\(W_{(d)}=U^{(d)} U^{(-d)T}\)</span></p>
<p>将<span class="math inline">\(\mathcal{L}\)</span> 中的 <span
class="math inline">\(L2-norm\)</span> 范数惩罚项按<span
class="math inline">\(mode-d\)</span> 模式矩阵拆分，即将</p>
<ul>
<li><span class="math inline">\(\Phi(\mathcal{W})=\frac{\lambda}{2}\|
[[U^{(1)},\cdots,U^{(M)}]] \|^2_{Fro}\)</span></li>
</ul>
<p>修改为</p>
<ul>
<li><span
class="math inline">\(\Phi(\mathcal{W})=\frac{\lambda}{2}\sum\limits^M_{d=1}
\|U^{(d)}\|^2 _{Fro}\)</span></li>
<li><span class="math inline">\(\frac{\lambda}{2} \|U^{(d)}\|^2
_{Fro}=\frac{\lambda}{2}\|vec(U^{(d)})\|^2\)</span></li>
</ul>
<p>则</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\mathcal{L}_{d} (U^{(d)},b) &amp;= \frac{1}{2} \sum\limits^N_{i=1}(y_i -
\text{Tr}(U^{(d)} \tilde{X}_{i(d)}^T)-b)^2 + \frac{\lambda}{2}
\|U^{(d)}\|^2 _{Fro}\\
&amp;= \frac{1}{2} \sum\limits^N_{i=1}(y_i
-  vec(\tilde{X}_{i(d)})^Tvec(U^{(d)})
-b)^2+\frac{\lambda}{2}\|vec(U^{(d)})\|^2 \\
&amp;=\frac{1}{2} \|
y-[vec(\tilde{X}_{1(d)}),\cdots,vec(\tilde{X}_{N(d)} )]^T
vec(U^{(d)})-N*b\|^2+\frac{\lambda}{2}\|vec(U^{(d)})\|^2\\
&amp;=\frac{1}{2} \|
y-[b,vec(\tilde{X}_{1(d)}),\cdots,vec(\tilde{X}_{N(d)} )]^T
[1,vec(U^{(d)})^T]^T\|^2+\frac{\lambda}{2}\|vec(U^{(d)})\|^2\\
&amp;=\frac{1}{2} \|
y-\Phi\hat\beta^{(d)}\|^2+\frac{\lambda}{2}\|\hat\beta^{(d)}\|^2
\end{aligned}
\end{equation}\]</span></p>
<p>注：为了获得目标函数坐标下降法的封闭解，这里添加了对常数偏置项的惩罚，这样获得的偏置系数估计是不准确的，在使用的时候需要额外的处理。其中</p>
<ul>
<li><p><span class="math inline">\(\Phi=
[vec(\tilde{X}_{1(d)}),\cdots,vec(\tilde{X}_{N(d)} )]^T\in\mathbb{R}^{N
\times I_dR }\)</span></p></li>
<li><p><span class="math inline">\(\hat\beta^{(d)}= [1,vec(U^{(d)})^T]^T
\in \mathbb{R}^{I_dR+1}\)</span></p></li>
<li><p><span class="math inline">\(U^{(d)} \in \mathbb{R}^{I_d \times
R}\)</span></p></li>
<li><p><span class="math inline">\(U^{(-d)} \in \mathbb{R}^{S_d \times
R}\)</span></p></li>
<li><p><span class="math inline">\(X_{i(d)}U^{(-d)} \in \mathbb{R}^{I_d
\times S_d}\)</span></p></li>
<li><p><span class="math inline">\(\tilde{X}_{i(d)} = X_{i(d)}U^{(-d)}
\in \mathbb{R}^{I_d \times R}\)</span></p></li>
</ul>
<p>这样关于权重参数张量的对其使用CP分解的矩阵的目标函数就转换为标准的岭回归问题了，则</p>
<ul>
<li><span class="math inline">\(\hat\beta^{(d)} = (\Phi^T \Phi + \lambda
I)^{-1} \Phi^{T}y\)</span></li>
<li><span class="math inline">\(U^{(d)} = reshape (\hat\beta^{(d)},[-1,
R])\)</span></li>
<li>使用<span class="math inline">\([[ U^{(1)}, U^{(2)},\cdots
U^{(M)}]]\)</span>重构<span
class="math inline">\(\mathcal{W}\)</span></li>
</ul>
<p>在不使用惩罚项修正权重参数的时候，即张量形式的最小二乘回归中偏置<span
class="math inline">\(b\)</span> 直接使用上式得出，在使用<span
class="math inline">\(L2-norm\)</span>
修正权重参数的时候由于偏置权重参数同时被修正，这导致常数偏置项的错误。所以TRR的常数偏置应该使用无偏置回归的残差平均值获得，也就是使用无偏置回归模型的残差平均值计算偏置，即：</p>
<ul>
<li><span class="math inline">\(b = \frac{1}{N} (y_i - \langle
\mathcal{X},\mathcal{W} \rangle )\)</span></li>
</ul>
<h2 id="global-dimensionality-reduction">3、Global Dimensionality
Reduction</h2>
<h3 id="符号-2">3.1 符号</h3>
<p><span class="math inline">\(x_i\)</span> 标量</p>
<p><span class="math inline">\(x\)</span> 列向量</p>
<p><span class="math inline">\(x^T\)</span> 行向量</p>
<p><span class="math inline">\(X\)</span> 矩阵</p>
<p><span class="math inline">\(I\)</span> 单位矩阵</p>
<p>张量 <span class="math inline">\(\mathcal{X}\in \mathbb{R}^{I_1
\times I_2 \times I_3 }\)</span></p>
<h3 id="tensor-transformation">3.2 Tensor Transformation</h3>
<p>Tucker Decomposition</p>
<ul>
<li><span class="math inline">\(\mathcal{X}_i = \mathcal{C}_i \times_1
U^{i}_1 \times_2 U^{i}_2 \times U^i_3\)</span></li>
</ul>
<p>其中</p>
<p><span class="math inline">\(\mathcal{X}_i \in \mathbb{R}^{I_1 \times
I_2 \times I_3 }\)</span></p>
<p><span class="math inline">\(\mathcal{C}_i \in \mathbb{R}^{J_1 \times
J_2 \times J_3 }\)</span></p>
<p><span class="math inline">\(U_k^{i} \in \mathbb{R}^{I_k\times R_k
}\)</span></p>
<p>并假设每个因子矩阵<span
class="math inline">\(U^i_k\)</span>描述一个信息源或信息模式，如企业特定、事件特定和情绪特定。核心张量<span
class="math inline">\(\mathcal{C}_1\)</span>表示由张量<span
class="math inline">\(\mathcal{X}_i\)</span>三中模型之间的相关程度。在Tucker
分解之后，最小化如下目标函数获得用于修正<span
class="math inline">\(U^i_k\)</span>相关矩阵<span
class="math inline">\(V_k \in \mathbb{R}^{I_k \times J_k}\)</span></p>
<ul>
<li><span class="math inline">\(\min \limits _{V_k} J(V_k) =
\frac{\sum\limits^N_{i=1}\sum\limits^N_{j=i}\|V_k^TU^i_k - V^T_kU^j_k
\|^2w_{i,j}}{\sum\limits^{N}_{i=1}\|
V^T_kU^i_K\|^2d_{i,i}}\)</span></li>
</ul>
<p>其中 <span
class="math inline">\(W\)</span>是一个加权的上三角矩阵，捕获张量序列<span
class="math inline">\(\mathcal{X}_i\)</span>的多维关联，其中元素<span
class="math inline">\(w_{i,j}\)</span>表征两个相似收益率模式的训练集：</p>
<ul>
<li><span class="math inline">\(w_{i,j} = \begin{cases}1,\ if\ \ i \le j
\ and \ \|y_i -y_j \| \le 5\% \\0, \ otherwise \end{cases}\)</span></li>
</ul>
<p>在Tucker分解中，因子矩阵<span
class="math inline">\(U^i_k\)</span>仅保留张量<span
class="math inline">\(\mathcal{X_i}\)</span>内的各种信息模式之间的固有关联。
为了捕获各种模式间的动态关系，对目标函数<span
class="math inline">\(J(V_k)\)</span>进行优化，以确定校正因子<span
class="math inline">\(V_k\)</span>以调整<span
class="math inline">\(U_i^k|^N_{i=1}\)</span>。目标函数试图降低相似收益率的因子张量的<span
class="math inline">\(mode\ k\)</span>
因子矩阵差异，同时最大程度保留<span class="math inline">\(mode\
k\)</span> 因子矩阵的原始信息。<span class="math inline">\(\|V_k^TU^i_k
- V^T_kU^j_k \|^2w_{i,j}\)</span>
通过最小化具最大相似程度(95%)因子矩阵<span
class="math inline">\(U^i_k\)</span> 和<span
class="math inline">\(U^j_k\)</span>
的获得修正张量分解因子矩阵的修正矩阵<span
class="math inline">\(V_k^T\)</span>。使用<span
class="math inline">\(w_{i,j}\)</span> 定义收益率相似股票<span
class="math inline">\(y_i\)</span> 和 <span
class="math inline">\(y_j\)</span>之间的差异来决断因子矩阵<span
class="math inline">\(U^i_k\)</span> 和<span
class="math inline">\(U^j_k\)</span>
之间的差异。为了避免过度修正特性模式因子矩阵<span
class="math inline">\(U_k^i\)</span> 这里最大化因子矩阵的方差。</p>
<ul>
<li><span class="math inline">\(var(x) = \sum (x_i -\mu)^2
p_i\)</span></li>
<li><span class="math inline">\(\mu = \sum x_i p_i\)</span></li>
</ul>
<p>可以使用<strong>图谱论(Spectral Graph Theory)</strong>
从对角矩阵<span class="math inline">\(D\)</span>估计概率<span
class="math inline">\(p_i\)</span>
最大化方差可以进行如下改写，即目标函数的分母。</p>
<ul>
<li><span class="math inline">\(var(V^T_kU_k)= \sum\limits^{N}_{i=1}\|
V^T_kU^i_K\|^2d_{i,i}\)</span></li>
</ul>
<p>假设<span class="math inline">\(V^T_ k
U_k\)</span>是张量子空间中的一个均值为0随机变量矩阵，令<span
class="math inline">\(A^i = V^T_k U_k^i\)</span> 则信息模式<span
class="math inline">\(mode \ k\)</span>的目标函数改写为：</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
J(V_k) &amp;= \frac{\sum\limits^N_{i=1}\sum\limits^N_{j=i}\|A^i - A^j
\|^2 w_{i,j}}{\sum\limits ^{N}_{i=1} \|A^i \|^2d_{i,i}}\\
&amp;=\frac{\sum\limits^N_{i=1}\sum\limits^N_{j=i}trace((A^i - A^j)(A^i
-A^j)^T) w_{i,j}}{\sum\limits ^{N}_{i=1} trace(A^i A^{iT})d_{i,i}}\\
&amp;=\frac{\sum\limits^N_{i=1}\sum\limits^N_{j=i}trace(A^iA^{iT}+A^jA^{jT}-A^iA^{jT}-A^jA^{iT}
)w_{i,j}}{\sum\limits ^{N}_{i=1} trace(A^i A^{iT})d_{i,i}}\\
&amp;=\frac{trace(\sum\limits^N_{i=1}A^iA^{iT}d_{i,i} -
\sum\limits^N_{i=1}\sum\limits^N_{j=1}A^iA^{jT}w_{i,j})}{trace(\sum\limits^N_{i}AA^{iT}d_{i,i})}
\end{aligned}
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(\| A\|^2 = \sum\limits^m_{i=1}
\sum\limits^m_{j=1}\| a_{i,j}\|^2= \left&lt;A,A\right&gt; =
trace(AA^T)\)</span></p>
<p>令</p>
<ul>
<li><span class="math inline">\(D_U = \sum\limits
^N_{i=1}d_{i,i}U^{i}U^{iT}\)</span></li>
<li><span class="math inline">\(W_U =
\sum\limits^N_{i=1}\sum\limits^N_{j=i}w_{i,j}U^iU^{jT}\)</span></li>
</ul>
<p>则目标函数可以改写为：</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
J(V_k)&amp;=\frac{trace(\sum\limits^N_{i=1}A^iA^{iT}d_{i,i} -
\sum\limits^N_{i=1}\sum\limits^N_{j=1}A^iA^{jT}w_{i,j})}{trace(\sum\limits^N_{i}AA^{iT}d_{i,i})}\\
&amp;= \frac{trace(\sum\limits^N_{i=1}V^T_kU^iU^{iT}V_kd_{i.i} -
\sum\limits^N_{i=1}\sum\limits^N_{j=i}V^T_kU^{i}U^{jT}V_kw_{i,j})}{trace(\sum\limits^N_{i}V^T_kU^iU^{iT}V_kd_{i,i})}\\
&amp;=\frac{trace(V^T_k(\sum\limits^N_{i=1}d_{i,i}U^iU^{iT})V_k -
V^T_k(\sum\limits^N_{i=1}\sum\limits^N_{j=1}w_{i,j}U^iU^{j,T})V_k)}{trace(V^T_k(\sum\limits^N_i
d_{i,i} U^i U^{iT})V_k)}\\
&amp;=\frac{trace (V^T_kD_UV_k-V^T_kW_UV_k)}{trace(V_k^TD_UV_k)}
\end{aligned}
\end{equation}\]</span></p>
<p>添加约束条件 <span class="math inline">\(trace(V^TD_UV)=1\)</span>
获得目标函数的唯一优化结果，这是获取因子矩阵对应的修正矩阵的优化问题转化为：</p>
<ul>
<li><span class="math inline">\(min\ J(V) =
trace(V^TD_UV-V^TW_UV)\)</span></li>
<li><span class="math inline">\(s.t. \ trace(V^TD_UV)=1\)</span></li>
</ul>
<p>通过如下算法即可获得修正矩阵，通过使用Tucker分解和修正矩阵将原始输入因子矩阵体积进行压缩</p>
<ul>
<li><p><span class="math inline">\(\mathcal{X}\in\mathbb{R}^{I_1 \times
I_2 \times I_3} \to \bar{\mathcal{X}} \in \mathbb{R}^{J_1 \times J_2
\times J_3}\)</span></p></li>
<li><p><span class="math inline">\(\bar{\mathcal{X}_i} = \mathcal{C}_i
\times_1 (V^T_1U^i_1) \times_2 (V^T_2U^i_2) \times_3
(V^T_3U^i_3)\)</span></p></li>
</ul>
<p><img src="GDR.PNG" width="100%" height="100%" title="GDR" alt="GDR"/></p>
<h3 id="sum-循环向量化">3.3 <span class="math inline">\(\sum\)</span>
循环向量化</h3>
<p><span class="math inline">\(U^i_k\in\mathbb{R}^{batch \times I_k
\times J_k}\)</span></p>
<p><span class="math inline">\(U^{iT}_k\in\mathbb{R}^{batch \times J_k
\times I_k}\)</span></p>
<p><span class="math inline">\(M^i_k = U^i_k U^{iT}_k\in
\mathbb{R}^{batch \times I_k \times I_k}\)</span></p>
<p><span class="math inline">\(M^i_{k(0)} \in \mathbb{R}^{batch \times
I_kI_k}\)</span></p>
<p>令 <span class="math inline">\(N = batch\)</span></p>
<ul>
<li><span class="math inline">\(D_{U_k} = \sum\limits
^N_{i=1}d_{i,i}U^{i}_kU^{iT}_k \\=\sum\limits ^N_{i=1}d_{i,i}M^i_k\\=
mat \{(vec(diag(D)))^T M_{k(0)}\}\)</span></li>
<li><span class="math inline">\(W_{U_k} = \sum\limits^N_{i=1}\sum\limits
^N_{j=1}w_{i,j}U^i_kU^{jT}_k\\= \sum\limits^N_{i=1}(\sum\limits
^N_{j=i}w_{i,j})U^i_kU^{jT}_k\\= \sum\limits^N_{i=1}(\sum\limits
^N_{j=i}w_{i,j})U^i_kU^{jT}_k\\=\sum\limits^N_{i=1}w_iU^i_kU^{jT}_k\\=\sum\limits^N_{i=1}w_i
M^i_{k}\\=mat\{w M_{k(0)} \}\)</span></li>
</ul>
<p>其中</p>
<ul>
<li><span class="math inline">\(w_{i,j} = \begin{cases}1,\ if\ \ i \le j
\ and \ \|y_i -y_j \| \le 5\% \\0, \ otherwise \end{cases}\)</span></li>
<li><span class="math inline">\(\sum\limits ^N_{j=1}w_{i,j} = w_i=
sum(w_{i,:}, axis=1)\)</span></li>
</ul>
<h1 id="引用">引用</h1>
<ul>
<li><p>Li X, Zhou H, Li L. Tucker tensor regression and neuroimaging
analysis[J]. arXiv preprint arXiv:1304.5637, 2013.</p></li>
<li><p>Petersen K B, Pedersen M S. The matrix cookbook[J]. Technical
University of Denmark, 2008, 7: 15.</p></li>
<li><p>Li Q, Jiang L L, Li P, et al. Tensor-Based Learning for
Predicting Stock Movements[C]//AAAI. 2015: 1784-1790.</p></li>
<li><p>Kolda T G, Bader B W. Tensor decompositions and applications[J].
SIAM review, 2009, 51(3): 455-500.</p></li>
<li><p>Guo W, Kotsia I, Patras I. Tensor learning for regression[J].
IEEE Transactions on Image Processing, 2012, 21(2): 816-827.</p></li>
<li><p>Tibshirani R. Modern regression 1: Ridge regression[C]//Data
Mining. 2013, 36: 462-662.</p></li>
<li><p>Figueiredo M A T. Lecture Notes on Linear Regression[J].</p></li>
<li><p>Nel D G. On matrix differentiation in statistics[J]. South
African Statistical Journal, 1980, 14(2): 137-193.</p></li>
<li><p>Dullemond K, Peeters K. Introduction to Tensor calculus[J]. Kees
Dullemond and Kasper Peeters, 1991.</p></li>
<li><p>Zhao Q, Caiafa C F, Mandic D P, et al. Higher order partial least
squares (HOPLS): a generalized multilinear regression method[J]. IEEE
transactions on pattern analysis and machine intelligence, 2013, 35(7):
1660-1673.</p></li>
<li><p>Li Q, Chen Y, Jiang L L, et al. A tensor-based information
framework for predicting the stock market[J]. ACM Transactions on
Information Systems (TOIS), 2016, 34(2): 11.</p></li>
</ul>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>稻草人
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://dybeta2021.github.io/2021/05/20/tesnor_ridge_regression/" title="Tesnor-Ridge-Regression">https://dybeta2021.github.io/2021/05/20/tesnor_ridge_regression/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/MultiFactor/" rel="tag"># MultiFactor</a>
              <a href="/tags/LinearRegression/" rel="tag"># LinearRegression</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/05/20/Black-Scholes-Model/" rel="prev" title="Black-Scholes Model">
      <i class="fa fa-chevron-left"></i> Black-Scholes Model
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/05/24/wing-model/" rel="next" title="Wing-Model Volatility Skew Manager">
      Wing-Model Volatility Skew Manager <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%99%84%E5%BD%95"><span class="nav-number">1.</span> <span class="nav-text">附录</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%90%91%E9%87%8F-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-number">1.1.</span> <span class="nav-text">1、向量-线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%A6%E5%8F%B7"><span class="nav-number">1.1.1.</span> <span class="nav-text">1.1、符号</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#least-squares-regression"><span class="nav-number">1.1.2.</span> <span class="nav-text">1.2、Least Squares Regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#proof-fracpartialbetatabetapartialbeta-a-atbeta"><span class="nav-number">1.1.3.</span> <span class="nav-text">1.3、 Proof:
\(\frac{\partial{\beta^TA\beta}}{\partial{\beta}} &#x3D;
(A + A^T)\beta\)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ridge-regression"><span class="nav-number">1.1.4.</span> <span class="nav-text">1.4、Ridge Regression</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%A0%E9%87%8F-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-number">1.2.</span> <span class="nav-text">2、张量-线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%A6%E5%8F%B7-1"><span class="nav-number">1.2.1.</span> <span class="nav-text">2.1 符号</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tensor-ridge-regression"><span class="nav-number">1.2.2.</span> <span class="nav-text">2.2 Tensor Ridge Regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%86%85%E7%A7%AF-inner-product"><span class="nav-number">1.2.3.</span> <span class="nav-text">2.3 内积 Inner Product</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9F%A9%E9%98%B5%E5%86%85%E7%A7%AF%E4%B8%8E%E8%BF%B9%E8%BF%90%E7%AE%97"><span class="nav-number">1.2.4.</span> <span class="nav-text">2.4 矩阵内积与迹运算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#frobenius-norm"><span class="nav-number">1.2.5.</span> <span class="nav-text">2.5 Frobenius norm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#cp%E5%88%86%E8%A7%A3"><span class="nav-number">1.2.6.</span> <span class="nav-text">2.6 CP分解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#trr%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0"><span class="nav-number">1.2.7.</span> <span class="nav-text">2.7 TRR目标函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#global-dimensionality-reduction"><span class="nav-number">1.3.</span> <span class="nav-text">3、Global Dimensionality
Reduction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%A6%E5%8F%B7-2"><span class="nav-number">1.3.1.</span> <span class="nav-text">3.1 符号</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tensor-transformation"><span class="nav-number">1.3.2.</span> <span class="nav-text">3.2 Tensor Transformation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sum-%E5%BE%AA%E7%8E%AF%E5%90%91%E9%87%8F%E5%8C%96"><span class="nav-number">1.3.3.</span> <span class="nav-text">3.3 \(\sum\)
循环向量化</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BC%95%E7%94%A8"><span class="nav-number">2.</span> <span class="nav-text">引用</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">稻草人</p>
  <div class="site-description" itemprop="description">风物长宜放眼量</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">67</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">61</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/dybeta2021" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;dybeta2021" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:dybeta2021@163.com" title="E-Mail → mailto:dybeta2021@163.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">稻草人</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>
博客全站共294.5k字
        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>

<script src="/js/utils.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>









<script>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>




  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
